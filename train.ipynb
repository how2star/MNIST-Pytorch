{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 检查训练设备，优先使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 1. 数据预处理和加载\n",
    "# 对训练集进行数据增强与标准化变换\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),                      # 随机旋转图像\n",
    "    transforms.RandomHorizontalFlip(),                  # 随机水平翻转\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),   # 随机平移\n",
    "    transforms.ToTensor(),  # 图像 -> Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化图像，使均值为0.5，标准差为0.5\n",
    "])\n",
    "\n",
    "# 对测试集进行标准化变换\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 下载并加载训练集和测试集\n",
    "full_train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=train_transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=test_transform, download=True)\n",
    "\n",
    "# 进一步地将训练集随机划分为训练集和验证集\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# 将数据集封装到DataLoader中，以便批量加载\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)   # 训练集，批量大小64\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)      # 验证集，批量大小64\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)    # 测试集，批量大小64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化可以使得输入数据的分布更加统一，避免某些特征因为数值较大或较小而主导了训练过程。\n",
    "\n",
    "(0.5, 0.5) 是一种常见的归一化策略，它把像素值映射到 [-1, 1] 的范围\n",
    "\n",
    "shuffle设置为True，在每个 epoch 开始时，数据集中的样本顺序会被随机打乱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 定义CNN模型\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 调用 super() , 确保父类的构造函数也被调用\n",
    "        super(CNN, self).__init__()\n",
    "        # 第一层卷积：输入通道1（灰度图像），输出通道32，卷积核大小3x3\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # 第二层卷积：输入通道32，输出通道64，卷积核大小3x3\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # 最大池化层：核大小2x2，步长2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # 全连接层1：将特征图展平到64*7*7，输出128个特征\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        # 全连接层2：输出10个特征（对应10个类别,即0~9）\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 第一层卷积、ReLU激活和池化\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        # 第二层卷积、ReLU激活和池化\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        # 将特征展平为1维向量，以便输入到全连接层\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        # 第一个全连接层和ReLU激活\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # 第二个全连接层（输出层）\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 实例化CNN模型\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，适用于多分类任务\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # 使用Adam优化器，learning rate设为0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/750], Loss: 1.3440\n",
      "Epoch [1/5], Step [200/750], Loss: 0.6444\n",
      "Epoch [1/5], Step [300/750], Loss: 0.4802\n",
      "Epoch [1/5], Step [400/750], Loss: 0.3948\n",
      "Epoch [1/5], Step [500/750], Loss: 0.3541\n",
      "Epoch [1/5], Step [600/750], Loss: 0.2990\n",
      "Epoch [1/5], Step [700/750], Loss: 0.3153\n",
      "----------------------------------------------\n",
      "Epoch [1/5] completed in 22.04 seconds.\n",
      "Validation Loss: 0.2970, Accuracy: 89.48%\n",
      "----------------------------------------------\n",
      "Epoch [2/5], Step [100/750], Loss: 0.2528\n",
      "Epoch [2/5], Step [200/750], Loss: 0.2505\n",
      "Epoch [2/5], Step [300/750], Loss: 0.2256\n",
      "Epoch [2/5], Step [400/750], Loss: 0.2111\n",
      "Epoch [2/5], Step [500/750], Loss: 0.2031\n",
      "Epoch [2/5], Step [600/750], Loss: 0.1853\n",
      "Epoch [2/5], Step [700/750], Loss: 0.1852\n",
      "----------------------------------------------\n",
      "Epoch [2/5] completed in 125.01 seconds.\n",
      "Validation Loss: 0.1670, Accuracy: 94.64%\n",
      "----------------------------------------------\n",
      "Epoch [3/5], Step [100/750], Loss: 0.1543\n",
      "Epoch [3/5], Step [200/750], Loss: 0.1433\n",
      "Epoch [3/5], Step [300/750], Loss: 0.1617\n",
      "Epoch [3/5], Step [400/750], Loss: 0.1529\n",
      "Epoch [3/5], Step [500/750], Loss: 0.1393\n",
      "Epoch [3/5], Step [600/750], Loss: 0.1435\n",
      "Epoch [3/5], Step [700/750], Loss: 0.1484\n",
      "----------------------------------------------\n",
      "Epoch [3/5] completed in 67.97 seconds.\n",
      "Validation Loss: 0.1347, Accuracy: 95.92%\n",
      "----------------------------------------------\n",
      "Epoch [4/5], Step [100/750], Loss: 0.1227\n",
      "Epoch [4/5], Step [200/750], Loss: 0.1356\n",
      "Epoch [4/5], Step [300/750], Loss: 0.1354\n",
      "Epoch [4/5], Step [400/750], Loss: 0.1211\n",
      "Epoch [4/5], Step [500/750], Loss: 0.1140\n",
      "Epoch [4/5], Step [600/750], Loss: 0.1231\n",
      "Epoch [4/5], Step [700/750], Loss: 0.1096\n",
      "----------------------------------------------\n",
      "Epoch [4/5] completed in 31.49 seconds.\n",
      "Validation Loss: 0.1228, Accuracy: 96.05%\n",
      "----------------------------------------------\n",
      "Epoch [5/5], Step [100/750], Loss: 0.1243\n",
      "Epoch [5/5], Step [200/750], Loss: 0.1013\n",
      "Epoch [5/5], Step [300/750], Loss: 0.1117\n",
      "Epoch [5/5], Step [400/750], Loss: 0.1153\n",
      "Epoch [5/5], Step [500/750], Loss: 0.1152\n",
      "Epoch [5/5], Step [600/750], Loss: 0.1057\n",
      "Epoch [5/5], Step [700/750], Loss: 0.0959\n",
      "----------------------------------------------\n",
      "Epoch [5/5] completed in 29.01 seconds.\n",
      "Validation Loss: 0.0986, Accuracy: 96.97%\n",
      "----------------------------------------------\n",
      "##############################################\n",
      "Total training time: 295.23 seconds\n",
      "Accuracy of the model on the test images: 98.36%\n",
      "##############################################\n"
     ]
    }
   ],
   "source": [
    "# 4. 开始记录训练时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 训练和验证模型\n",
    "num_epochs = 5  # 训练轮次\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # 记录每个epoch的总损失\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # 将图像和标签移动到训练设备\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # 将梯度清零，以便计算新的梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播，计算模型输出\n",
    "        outputs = model(images)\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        # 反向传播，计算梯度\n",
    "        loss.backward()\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 累加损失值\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每100个小批次打印一次损失\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # 记录每个epoch的训练时间\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print('----------------------------------------------')\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed in {epoch_duration:.2f} seconds.\")\n",
    "\n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss / len(val_loader):.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "    print('----------------------------------------------')\n",
    "\n",
    "# 训练总时长\n",
    "total_training_time = time.time() - start_time\n",
    "print('##############################################')\n",
    "print(f'Total training time: {total_training_time:.2f} seconds')\n",
    "\n",
    "# 5. 只保存模型的参数\n",
    "torch.save(model.state_dict(), './model/myCNNmodel_MNIST.pth')\n",
    "\n",
    "# 测试模型\n",
    "model.eval()\n",
    "correct = 0  # 记录正确分类的样本数\n",
    "total = 0    # 记录测试集的总样本数\n",
    "\n",
    "# 在测试过程中，不需要计算梯度，使用torch.no_grad()可以加速测试\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # 将图像和标签移动到GPU\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # 获取模型输出\n",
    "        outputs = model(images)\n",
    "        # 获取最大值对应的类别\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # 累加正确分类的样本数\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 打印模型的分类准确率\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test images: {test_accuracy:.2f}%')\n",
    "print('##############################################')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
